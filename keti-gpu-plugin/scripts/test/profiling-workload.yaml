---
apiVersion: v1
kind: Pod
metadata:
  name: profile-workload-a
  namespace: default
  annotations:
    nvidia.com/gpumem: "4000"
    nvidia.com/gpucores: "30"
spec:
  nodeName: edge-gpu-232
  containers:
    - name: profiler
      image: nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04
      command:
        - /bin/bash
        - -c
        - |
          # Install nsight-systems
          apt-get update -qq 2>/dev/null
          apt-get install -y -qq nsight-systems-cli 2>/dev/null || echo "nsys install skipped"

          cat > /tmp/compute.cu << 'CUDAEOF'
          #include <stdio.h>
          #include <cuda_runtime.h>
          #include <cublas_v2.h>

          int main() {
              const int N = 4096;
              size_t size = N * N * sizeof(float);
              float *d_A, *d_B, *d_C;

              printf("[ProfileA-30%%] Starting profiled GPU compute...\n");
              fflush(stdout);

              cudaMalloc(&d_A, size);
              cudaMalloc(&d_B, size);
              cudaMalloc(&d_C, size);

              float *h_A = (float*)malloc(size);
              float *h_B = (float*)malloc(size);
              for(int i = 0; i < N*N; i++) {
                  h_A[i] = (float)(i % 100) / 100.0f;
                  h_B[i] = (float)((i+1) % 100) / 100.0f;
              }
              cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
              cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
              free(h_A); free(h_B);

              cublasHandle_t handle;
              cublasCreate(&handle);
              float alpha = 1.0f, beta = 0.0f;

              // Warm up
              for(int i = 0; i < 10; i++) {
                  cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                              N, N, N, &alpha, d_A, N, d_B, N, &beta, d_C, N);
              }
              cudaDeviceSynchronize();

              printf("[ProfileA-30%%] Running 100 iterations...\n");
              fflush(stdout);

              // Profile this section
              for(int i = 0; i < 100; i++) {
                  cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                              N, N, N, &alpha, d_A, N, d_B, N, &beta, d_C, N);
              }
              cudaDeviceSynchronize();

              printf("[ProfileA-30%%] Done!\n");

              cublasDestroy(handle);
              cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
              return 0;
          }
          CUDAEOF

          nvcc -o /tmp/compute /tmp/compute.cu -lcublas 2>&1

          # Try nsys if available, otherwise use nvprof or direct run
          if command -v nsys &> /dev/null; then
              nsys profile --stats=true --output=/tmp/profile_a /tmp/compute
              nsys stats /tmp/profile_a.nsys-rep
          else
              echo "[ProfileA] nsys not available, using nvidia-smi monitoring"
              /tmp/compute &
              PID=$!
              for i in $(seq 1 10); do
                  nvidia-smi --query-compute-apps=pid,used_memory,name --format=csv,noheader 2>/dev/null
                  nvidia-smi --query-gpu=utilization.gpu,utilization.memory,sm --format=csv,noheader 2>/dev/null
                  sleep 0.5
              done
              wait $PID
          fi

          # Keep running for observation
          sleep 300
      resources:
        limits:
          nvidia.com/gpu: 1
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: profile-workload-b
  namespace: default
  annotations:
    nvidia.com/gpumem: "8000"
    nvidia.com/gpucores: "70"
spec:
  nodeName: edge-gpu-232
  containers:
    - name: profiler
      image: nvcr.io/nvidia/cuda:12.4.1-devel-ubuntu22.04
      command:
        - /bin/bash
        - -c
        - |
          cat > /tmp/compute.cu << 'CUDAEOF'
          #include <stdio.h>
          #include <cuda_runtime.h>
          #include <cublas_v2.h>

          int main() {
              const int N = 8192;
              size_t size = N * N * sizeof(float);
              float *d_A, *d_B, *d_C;

              printf("[ProfileB-70%%] Starting profiled GPU compute...\n");
              fflush(stdout);

              cudaMalloc(&d_A, size);
              cudaMalloc(&d_B, size);
              cudaMalloc(&d_C, size);

              float *h_A = (float*)malloc(size);
              float *h_B = (float*)malloc(size);
              for(int i = 0; i < N*N; i++) {
                  h_A[i] = (float)(i % 100) / 100.0f;
                  h_B[i] = (float)((i+1) % 100) / 100.0f;
              }
              cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
              cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
              free(h_A); free(h_B);

              cublasHandle_t handle;
              cublasCreate(&handle);
              float alpha = 1.0f, beta = 0.0f;

              // Warm up
              for(int i = 0; i < 10; i++) {
                  cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                              N, N, N, &alpha, d_A, N, d_B, N, &beta, d_C, N);
              }
              cudaDeviceSynchronize();

              printf("[ProfileB-70%%] Running 100 iterations...\n");
              fflush(stdout);

              // Profile this section
              for(int i = 0; i < 100; i++) {
                  cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                              N, N, N, &alpha, d_A, N, d_B, N, &beta, d_C, N);
              }
              cudaDeviceSynchronize();

              printf("[ProfileB-70%%] Done!\n");

              cublasDestroy(handle);
              cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
              return 0;
          }
          CUDAEOF

          nvcc -o /tmp/compute /tmp/compute.cu -lcublas 2>&1

          echo "[ProfileB] Running with nvidia-smi monitoring"
          /tmp/compute &
          PID=$!
          for i in $(seq 1 10); do
              nvidia-smi --query-compute-apps=pid,used_memory,name --format=csv,noheader 2>/dev/null
              nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv,noheader 2>/dev/null
              sleep 0.5
          done
          wait $PID

          sleep 300
      resources:
        limits:
          nvidia.com/gpu: 1
  restartPolicy: Never
