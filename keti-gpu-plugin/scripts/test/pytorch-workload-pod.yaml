apiVersion: v1
kind: Pod
metadata:
  name: pytorch-workload
  namespace: default
  annotations:
    nvidia.com/gpumem: "8000"
    nvidia.com/gpucores: "40"
spec:
  nodeName: edge-gpu-232
  containers:
    - name: pytorch
      image: nvcr.io/nvidia/pytorch:24.10-py3
      command:
        - python3
        - -c
        - |
          import sys
          import time
          import torch

          print("[Workload] Starting PyTorch GPU workload (120s)...")
          device = torch.device("cuda:0")

          # GPU 정보 출력
          print(f"[Workload] GPU: {torch.cuda.get_device_name(0)}")
          print(f"[Workload] CUDA: {torch.version.cuda}")

          # 큰 행렬 생성 - 메모리 사용
          size = 8192
          print(f"[Workload] Allocating {size}x{size} tensors...")
          a = torch.randn(size, size, device=device)
          b = torch.randn(size, size, device=device)

          print(f"[Workload] GPU Memory: {torch.cuda.memory_allocated()/1024/1024:.0f} MB")

          start = time.time()
          iterations = 0

          while time.time() - start < 120:
              c = torch.matmul(a, b)
              torch.cuda.synchronize()
              iterations += 1

              if iterations % 20 == 0:
                  elapsed = time.time() - start
                  mem = torch.cuda.memory_allocated()/1024/1024
                  print(f"[Workload] {elapsed:.1f}s - {iterations} iters - {mem:.0f} MB")

          print(f"[Workload] Done! {iterations} iterations")
      resources:
        limits:
          nvidia.com/gpu: 1
  restartPolicy: Never
