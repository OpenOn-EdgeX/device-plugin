apiVersion: v1
kind: Pod
metadata:
  name: gpu-workload
  namespace: default
  annotations:
    nvidia.com/gpumem: "8000"
    nvidia.com/gpucores: "40"
spec:
  nodeName: edge-gpu-232
  containers:
    - name: workload
      image: nvidia/cuda:12.4.1-devel-ubuntu22.04
      command:
        - /bin/bash
        - -c
        - |
          echo "[Workload] Installing Python and cupy..."
          apt-get update -qq && apt-get install -y -qq python3 python3-pip > /dev/null
          pip install cupy-cuda12x -q

          echo "[Workload] Starting GPU memory test..."
          python3 << 'PYEOF'
          import cupy as cp
          import time

          print("[Workload] Starting GPU workload (120s)...")

          # GPU 메모리 할당 - 8GB
          size = 32768
          print(f"[Workload] Allocating {size}x{size} matrices...")
          a = cp.random.randn(size, size, dtype=cp.float32)
          b = cp.random.randn(size, size, dtype=cp.float32)

          mempool = cp.get_default_memory_pool()
          used_mb = mempool.used_bytes() / 1024 / 1024
          print(f"[Workload] GPU Memory allocated: {used_mb:.0f} MB")

          start = time.time()
          iterations = 0

          while time.time() - start < 120:
              c = cp.matmul(a, b)
              cp.cuda.Stream.null.synchronize()
              iterations += 1

              if iterations % 10 == 0:
                  elapsed = time.time() - start
                  used_mb = mempool.used_bytes() / 1024 / 1024
                  print(f"[Workload] {elapsed:.1f}s - {iterations} iters - {used_mb:.0f} MB")

          print(f"[Workload] Done! {iterations} iterations")
          PYEOF
      resources:
        limits:
          nvidia.com/gpu: 1
  restartPolicy: Never
