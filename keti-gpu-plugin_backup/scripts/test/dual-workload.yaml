---
apiVersion: v1
kind: Pod
metadata:
  name: workload-a
  namespace: default
  annotations:
    nvidia.com/gpumem: "4000"
    nvidia.com/gpucores: "30"
spec:
  nodeName: edge-gpu-232
  containers:
    - name: workload
      image: nvidia/cuda:12.4.1-devel-ubuntu22.04
      command:
        - /bin/bash
        - -c
        - |
          cat > /tmp/compute.cu << 'CUDAEOF'
          #include <stdio.h>
          #include <cuda_runtime.h>
          #include <cublas_v2.h>
          #include <unistd.h>

          int main() {
              const int N = 4096;  // 4096x4096 matrix = ~256MB per matrix
              size_t size = N * N * sizeof(float);
              float *d_A, *d_B, *d_C;
              size_t free_mem, total_mem;

              printf("[WorkloadA-30%%] Starting GPU compute (180s)...\n");
              fflush(stdout);

              // Allocate GPU memory
              cudaMalloc(&d_A, size);
              cudaMalloc(&d_B, size);
              cudaMalloc(&d_C, size);

              // Initialize with random data
              float *h_A = (float*)malloc(size);
              float *h_B = (float*)malloc(size);
              for(int i = 0; i < N*N; i++) {
                  h_A[i] = (float)(i % 100) / 100.0f;
                  h_B[i] = (float)((i+1) % 100) / 100.0f;
              }
              cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
              cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
              free(h_A); free(h_B);

              cudaMemGetInfo(&free_mem, &total_mem);
              printf("[WorkloadA-30%%] Memory: %zu MB used\n", (total_mem-free_mem)/1024/1024);
              fflush(stdout);

              // cuBLAS for matrix multiplication
              cublasHandle_t handle;
              cublasCreate(&handle);
              float alpha = 1.0f, beta = 0.0f;

              int elapsed = 0;
              while(elapsed < 180) {
                  // Matrix multiply: C = A * B (GPU intensive)
                  for(int i = 0; i < 50; i++) {
                      cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                                  N, N, N, &alpha, d_A, N, d_B, N, &beta, d_C, N);
                  }
                  cudaDeviceSynchronize();

                  cudaMemGetInfo(&free_mem, &total_mem);
                  printf("[WorkloadA-30%%] %ds - %zu MB - computing...\n",
                         elapsed, (total_mem-free_mem)/1024/1024);
                  fflush(stdout);
                  sleep(10);
                  elapsed += 10;
              }

              cublasDestroy(handle);
              cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
              printf("[WorkloadA-30%%] Done!\n");
              return 0;
          }
          CUDAEOF
          nvcc -o /tmp/compute /tmp/compute.cu -lcublas 2>&1 && /tmp/compute
      resources:
        limits:
          nvidia.com/gpu: 1
  restartPolicy: Never
---
apiVersion: v1
kind: Pod
metadata:
  name: workload-b
  namespace: default
  annotations:
    nvidia.com/gpumem: "8000"
    nvidia.com/gpucores: "70"
spec:
  nodeName: edge-gpu-232
  containers:
    - name: workload
      image: nvidia/cuda:12.4.1-devel-ubuntu22.04
      command:
        - /bin/bash
        - -c
        - |
          cat > /tmp/compute.cu << 'CUDAEOF'
          #include <stdio.h>
          #include <cuda_runtime.h>
          #include <cublas_v2.h>
          #include <unistd.h>

          int main() {
              const int N = 8192;  // 8192x8192 matrix = ~1GB per matrix
              size_t size = N * N * sizeof(float);
              float *d_A, *d_B, *d_C;
              size_t free_mem, total_mem;

              printf("[WorkloadB-70%%] Starting GPU compute (180s)...\n");
              fflush(stdout);

              // Allocate GPU memory
              cudaMalloc(&d_A, size);
              cudaMalloc(&d_B, size);
              cudaMalloc(&d_C, size);

              // Initialize with random data
              float *h_A = (float*)malloc(size);
              float *h_B = (float*)malloc(size);
              for(int i = 0; i < N*N; i++) {
                  h_A[i] = (float)(i % 100) / 100.0f;
                  h_B[i] = (float)((i+1) % 100) / 100.0f;
              }
              cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
              cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
              free(h_A); free(h_B);

              cudaMemGetInfo(&free_mem, &total_mem);
              printf("[WorkloadB-70%%] Memory: %zu MB used\n", (total_mem-free_mem)/1024/1024);
              fflush(stdout);

              // cuBLAS for matrix multiplication
              cublasHandle_t handle;
              cublasCreate(&handle);
              float alpha = 1.0f, beta = 0.0f;

              int elapsed = 0;
              while(elapsed < 180) {
                  // Matrix multiply: C = A * B (GPU intensive)
                  for(int i = 0; i < 50; i++) {
                      cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                                  N, N, N, &alpha, d_A, N, d_B, N, &beta, d_C, N);
                  }
                  cudaDeviceSynchronize();

                  cudaMemGetInfo(&free_mem, &total_mem);
                  printf("[WorkloadB-70%%] %ds - %zu MB - computing...\n",
                         elapsed, (total_mem-free_mem)/1024/1024);
                  fflush(stdout);
                  sleep(10);
                  elapsed += 10;
              }

              cublasDestroy(handle);
              cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
              printf("[WorkloadB-70%%] Done!\n");
              return 0;
          }
          CUDAEOF
          nvcc -o /tmp/compute /tmp/compute.cu -lcublas 2>&1 && /tmp/compute
      resources:
        limits:
          nvidia.com/gpu: 1
  restartPolicy: Never
